{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, random, argparse\n",
    "import pickle\n",
    "from collections import defaultdict as ddict\n",
    "import tensorflow as tf\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, randint\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set precision for numpy\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6ace170bb86f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/gids_bert_processed-test.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_pos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getData' is not defined"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "data = pickle.load(open('data/gids_bert_processed-test.pkl', 'rb'))\n",
    "\n",
    "padded, _, _ = getData(data['train'], data['max_pos'])\n",
    "\n",
    "count = 0\n",
    "for pad_pos1, pad_pos2, embeds, pad_alias, sub_type, obj_type, label in padded:\n",
    "    print(\"{0} = {1}\".format(count,pad_pos1))\n",
    "    #print(pad_pos1.dtype)\n",
    "    print(pad_pos2.shape)\n",
    "    print(pad_alias)\n",
    "    if count==3:\n",
    "        break\n",
    "    count+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padData(data, max_sent_in_bag, max_alias, max_alias_len, subj_max, obj_max, aliases_count):\n",
    "    dataset = []\n",
    "    countBags = 0\n",
    "    \n",
    "    for i, bag in enumerate(data):\n",
    "        embeds = bag['embeds']\n",
    "        aliases = bag['ProbY']\n",
    "        sub_type = bag['SubType']\n",
    "        obj_type = bag['ObjType']\n",
    "        \n",
    "        # padding embeds\n",
    "        while len(embeds) != max_sent_in_bag:\n",
    "            embeds.append(np.zeros(768))\n",
    "        \n",
    "        # padding alias\n",
    "        while len(aliases) != max_sent_in_bag:\n",
    "            aliases.append([0])\n",
    "              \n",
    "        pad_alias = np.zeros((len(aliases), max_alias), np.int32)\n",
    "        for i, alias in enumerate(aliases):\n",
    "            pad_alias[i, :len(alias)] = alias[:max_alias]\n",
    "        \n",
    "        # padding subject\n",
    "        while len(sub_type) != subj_max:\n",
    "            sub_type.append(0)\n",
    "            \n",
    "        # padding object\n",
    "        while len(obj_type) != obj_max:\n",
    "            obj_type.append(0)\n",
    "        \n",
    "        label = bag['Y']\n",
    "        \n",
    "        while len(label) > 1:\n",
    "            label.pop()\n",
    "        \n",
    "        dataset.append([embeds, pad_alias, sub_type, obj_type, label])\n",
    "        countBags += 1\n",
    "        \n",
    "    return dataset, countBags, aliases_count\n",
    "    \n",
    "\n",
    "def getData(data):\n",
    "    max_sent_in_bag, max_alias, max_alias_len, subj_max, obj_max, aliases_count = 0, 0, 0, 0, 0, 0 \n",
    "    \n",
    "    for i, bag in enumerate(data):\n",
    "        embeds = bag['embeds']\n",
    "        aliases = bag['ProbY']\n",
    "        sub_type = bag['SubType']\n",
    "        obj_type = bag['ObjType']\n",
    "        \n",
    "        max_sent_in_bag = max(max_sent_in_bag, len(embeds))\n",
    "        \n",
    "        max_alias = max(max_alias, len(aliases))\n",
    "        \n",
    "        for alias in aliases:\n",
    "            max_alias_len = max(max_alias_len, len(alias))\n",
    "            for a in alias:\n",
    "                aliases_count = max(aliases_count, a)\n",
    "            \n",
    "        subj_max = max(subj_max, len(sub_type))\n",
    "        obj_max = max(obj_max, len(obj_type))\n",
    "    \n",
    "    #print(\"MAX_SENT_IN_BAG: %d\" % max_sent_in_bag)\n",
    "    #print(\"MAX_ALIAS: %d\" % max_alias)\n",
    "    #print(\"MAX_ALIAS_LEN: %d\" % max_alias_len)\n",
    "    #print(\"SUBJ_MAX: %d\" % subj_max)\n",
    "    #print(\"aliases_count: %d\\n\" % aliases_count)\n",
    "    \n",
    "    return padData(data, max_sent_in_bag, max_alias, max_alias_len, subj_max, obj_max, aliases_count+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    batch_size = 512\n",
    "    type_dim = 150\n",
    "    alias_dim = 150\n",
    "    embedding_dim = 768\n",
    "    L2 = 0.001\n",
    "    regularizer = tf.keras.regularizers.l2(l=0.5 * (L2))\n",
    "\n",
    "    data = pickle.load(open('data/gids_bert_processed-test.pkl', 'rb'))\n",
    "\n",
    "    voc2id      = data['voc2id']\n",
    "    id2voc      = data['id2voc']\n",
    "    type2id     = data['type2id']\n",
    "    type_count  = len(data['type2id'])\n",
    "    num_class   = len(data['rel2id'])\n",
    "    max_pos = data['max_pos']\n",
    "    \n",
    "    print(\"getData Train\")\n",
    "    train, train_bags, alias_count = getData(data['train'])\n",
    "    \n",
    "    print(\"getData Test\")\n",
    "    validation, test_bags, alias_count = getData(data['dev'])\n",
    "    \n",
    "    print(\"getData Validation\")\n",
    "    test, dev_bags, alias_count = getData(data['test'])\n",
    "    \n",
    "    return train, validation, test, embedding_dim, num_class, alias_dim, regularizer, type_count, type_dim, batch_size, alias_count\n",
    "\n",
    "def get_chunks(data, batch_size):\n",
    "    count_bags = len(data)\n",
    "    for indx in range(0, count_bags, batch_size):\n",
    "        yield data[indx:min(indx + batch_size, count_bags)]\n",
    "        \n",
    "def create_batches(data, batch_size=64):\n",
    "    batches = []\n",
    "    for chunk in get_chunks(data, batch_size):\n",
    "        batch = ddict(list)\n",
    "        num = 0\n",
    "        \n",
    "        bert = []\n",
    "        alias = []\n",
    "        subj = []\n",
    "        obj = []\n",
    "        out = []\n",
    "        for i, bag in enumerate(chunk):\n",
    "            bert.append(bag[0])\n",
    "            alias.append(bag[1])\n",
    "            subj.append(bag[2])\n",
    "            obj.append(bag[3])\n",
    "            out.append(bag[4])\n",
    "        batches.append([bert, alias, subj, obj, out])\n",
    "        \n",
    "    return batches, len(batches)\n",
    "    \n",
    "def get_batches(batches):\n",
    "    while True:\n",
    "        for batch in batches:\n",
    "            yield ({'bert_input': np.array(batch[0], dtype='float32'), \n",
    "                    'alias': np.array(batch[1], dtype='int32'), \n",
    "                    'subj': np.array(batch[2], dtype='int32'), \n",
    "                    'obj': np.array(batch[3], dtype='int32')}, \n",
    "                   {'output': np.array(batch[4], dtype='int32')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train, validation, test, embedding_dim, num_class, alias_dim, regularizer, \n",
    "          type_count, type_dim, batch_size, max_pos, alias_count):\n",
    "    \n",
    "    # embed input\n",
    "    embed_input = tf.keras.Input(shape=(None, embedding_dim,), name=\"bert_input\", dtype='float32')\n",
    "    \n",
    "    # embed alias\n",
    "    alias_input = tf.keras.Input(shape=(None, None,), name=\"alias\", dtype='int32')\n",
    "    embed_alias = tf.keras.layers.Embedding(input_dim=alias_count, \n",
    "                                            output_dim=alias_dim, trainable=True,\n",
    "                                            embeddings_initializer=tf.keras.initializers.VarianceScaling(\n",
    "                                                scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"), \n",
    "                                            embeddings_regularizer=regularizer, name='alias_embed')(alias_input)\n",
    "    \n",
    "    alias_av = tf.math.reduce_sum(input_tensor=embed_alias, axis=2, keepdims=False, name='alias_mean') / tf.cast(tf.shape(embed_alias)[1], tf.float32)\n",
    "    \n",
    "    # sentence representations\n",
    "    sent_reps = tf.keras.layers.concatenate([embed_input, alias_av], axis=2, name='bert_alias_concat')\n",
    "    \n",
    "    \n",
    "    # Input subj-obj types\n",
    "    subj_input = tf.keras.layers.Input(shape=(None,), name=\"subj\", dtype='int32')\n",
    "    obj_input = tf.keras.layers.Input(shape=(None,), name=\"obj\", dtype='int32')\n",
    "    \n",
    "    # embed subj-obj types\n",
    "    embed_type = tf.keras.layers.Embedding(input_dim=type_count, output_dim=type_dim, \n",
    "                                           embeddings_regularizer=regularizer,\n",
    "                                           embeddings_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
    "                                               scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"), name='type_embed')\n",
    "    subj_embed = embed_type(subj_input)\n",
    "    obj_embed = embed_type(obj_input)\n",
    "    \n",
    "    # average of types representations\n",
    "    subj_type_av = tf.math.reduce_mean(input_tensor=subj_embed, axis=1, name='subj_mean')\n",
    "    obj_type_av = tf.math.reduce_mean(input_tensor=obj_embed, axis=1, name='obj_mean')\n",
    "    \n",
    "    # concatenate subject and object to one single representation\n",
    "    concat_type = tf.keras.layers.concatenate([subj_type_av, obj_type_av], axis=1, name='type_concat')\n",
    "    \n",
    "    # weights for querying attention layer \n",
    "    # samples are drawn from a uniform distribution within [-limit, limit], with limit = sqrt(3 * scale / n)\n",
    "    sent_atten_q = tf.keras.initializers.VarianceScaling(scale=1.0, \n",
    "                                                         mode=\"fan_avg\", \n",
    "                                                         distribution=\"uniform\")(shape=(1, 1, alias_dim+embedding_dim))  # alias_dim+\n",
    "    \n",
    "    # Bag Attention sentence level\n",
    "    bag_attention = tf.keras.layers.Attention(name='attention')([sent_atten_q, sent_reps])\n",
    "    \n",
    "    mean_bag_attention = tf.math.reduce_mean(bag_attention, axis=1)\n",
    "    \n",
    "    bag_reps = tf.keras.layers.concatenate([mean_bag_attention, concat_type], axis=1, name='bag_rep_type_concat')\n",
    "    \n",
    "    # fully connected\n",
    "    fc1 = tf.keras.layers.Dense(units={{choice([50, 60, 70, 80, 90, 100])}}, \n",
    "                                activation=\"relu\", \n",
    "                                kernel_regularizer=regularizer, name='fully1')(bag_reps)\n",
    "    \n",
    "    drop1 = tf.keras.layers.Dropout(rate={{uniform(0,1)}}, name='drop1')(fc1)\n",
    "    \n",
    "    fc2 = tf.keras.layers.Dense(units={{choice([5, 10, 25, 40, 50])}}, \n",
    "                                activation=\"relu\", \n",
    "                                kernel_regularizer=regularizer, name='fully2')(drop1)\n",
    "    \n",
    "    drop = tf.keras.layers.Dropout(rate={{uniform(0,1)}}, name='drop2')(fc2)\n",
    "    \n",
    "    # output layer\n",
    "    label = tf.keras.layers.Dense(num_class, activation='softmax', kernel_regularizer=regularizer, name='output')(drop)\n",
    "    \n",
    "    model = tf.keras.Model([embed_input, alias_input, subj_input, obj_input], label)\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    #nadam = tf.keras.optimizers.Nadam(lr={{uniform(0,1)}}, name='Nadam')\n",
    "    sgd = tf.keras.optimizers.SGD(lr={{uniform(0,1)}}, name='SGD')\n",
    "\n",
    "    '''choiceval = {{choice(['sgd'])}}\n",
    "    if choiceval == 'nadam':\n",
    "        optim = nadam\n",
    "    elif choiceval == 'sgd':'''\n",
    "    optim = sgd\n",
    "    \n",
    "    \n",
    "    model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                  optimizer=optim,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    train_batches, train_batches_len = create_batches(train, batch_size)\n",
    "    val_batches, val_batches_len = create_batches(validation, batch_size)\n",
    "    test_batches, test_batches_len = create_batches(test, batch_size)\n",
    "    \n",
    "    \n",
    "    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath=\"models/es_best.h5\", verbose=1, \n",
    "                                   save_best_only=True, save_weights_only=True)\n",
    "    earlystopper = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=40, verbose=1)\n",
    "    \n",
    "    history = model.fit_generator(get_batches(train_batches), steps_per_epoch=train_batches_len, \n",
    "                        epochs=1000, verbose=2, validation_steps=val_batches_len, \n",
    "                        validation_data=get_batches(val_batches), callbacks=[checkpointer, earlystopper])\n",
    "    \n",
    "    score, acc = model.evaluate_generator(generator=get_batches(test_batches), \n",
    "                                          steps=test_batches_len, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model, \n",
    "            'history.val_loss':history.history['val_loss'], \n",
    "            'history.val_acc': history.history['val_accuracy'],\n",
    "            'history.loss': history.history['loss'], \n",
    "            'history.acc': history.history['accuracy']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os, sys, random, argparse\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pickle\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from collections import defaultdict as ddict\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, randint\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import metrics\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import collections\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'units': hp.choice('units', [50, 60, 70, 80, 90, 100]),\n",
      "        'rate': hp.uniform('rate', 0,1),\n",
      "        'units_1': hp.choice('units_1', [5, 10, 25, 40, 50]),\n",
      "        'rate_1': hp.uniform('rate_1', 0,1),\n",
      "        'hidden_layers_number': hp.choice('hidden_layers_number', ['one', 'two', 'three']),\n",
      "        'units_2': hp.choice('units_2', [450, 600, 750, 900]),\n",
      "        'activation': hp.choice('activation', [\"tanh\", \"relu\", \"sigmoid\"]),\n",
      "        'rate_2': hp.uniform('rate_2', 0,1),\n",
      "        'units_3': hp.choice('units_3', [150, 300, 450]),\n",
      "        'activation_1': hp.choice('activation_1', [\"tanh\", \"relu\", \"sigmoid\"]),\n",
      "        'rate_3': hp.uniform('rate_3', 0,1),\n",
      "        'units_4': hp.choice('units_4', [50, 100, 150, 300]),\n",
      "        'activation_2': hp.choice('activation_2', [\"tanh\", \"relu\", \"sigmoid\"]),\n",
      "        'rate_4': hp.uniform('rate_4', 0,1),\n",
      "        'rate_5': hp.uniform('rate_5', 0,1),\n",
      "        'rate_6': hp.uniform('rate_6', 0,1),\n",
      "        'choiceval': hp.choice('choiceval', ['sgd']),\n",
      "    }\n",
      "\n",
      ">>> Functions\n",
      "   1: def padData(data, max_sent_in_bag, max_alias, max_alias_len, subj_max, obj_max, max_sent_size, aliases_count):\n",
      "   2:     dataset = []\n",
      "   3:     countBags = 0\n",
      "   4:     \n",
      "   5:     for i, bag in enumerate(data):\n",
      "   6:         embeds = bag['embeds']\n",
      "   7:         aliases = bag['ProbY']\n",
      "   8:         sub_type = bag['SubType']\n",
      "   9:         obj_type = bag['ObjType']\n",
      "  10:         pos1 = bag['Pos1']\n",
      "  11:         pos2 = bag['Pos2']\n",
      "  12:         \n",
      "  13:         while len(pos1) != max_sent_in_bag:\n",
      "  14:             pos1.append(np.zeros(max_sent_size))\n",
      "  15:             \n",
      "  16:         pad_pos1 = np.zeros((len(pos1), max_sent_size), np.int32)\n",
      "  17:         for i, pos in enumerate(pos1):\n",
      "  18:             pad_pos1[i, :len(pos)] = pos[:max_sent_size]\n",
      "  19:             \n",
      "  20:         while len(pos2) != max_sent_in_bag:\n",
      "  21:             pos2.append(np.zeros(max_sent_size))\n",
      "  22:         \n",
      "  23:         pad_pos2 = np.zeros((len(pos2), max_sent_size), np.int32)\n",
      "  24:         for i, pos in enumerate(pos2):\n",
      "  25:             pad_pos2[i, :len(pos)] = pos[:max_sent_size]\n",
      "  26:             \n",
      "  27:         # padding embeds\n",
      "  28:         while len(embeds) != max_sent_in_bag:\n",
      "  29:             embeds.append(np.zeros(768))\n",
      "  30:         \n",
      "  31:         # padding alias\n",
      "  32:         while len(aliases) != max_sent_in_bag:\n",
      "  33:             aliases.append([0])\n",
      "  34:               \n",
      "  35:         pad_alias = np.zeros((len(aliases), max_alias), np.int32)\n",
      "  36:         for i, alias in enumerate(aliases):\n",
      "  37:             pad_alias[i, :len(alias)] = alias[:max_alias]\n",
      "  38:         \n",
      "  39:         # padding subject\n",
      "  40:         while len(sub_type) != subj_max:\n",
      "  41:             sub_type.append(0)\n",
      "  42:             \n",
      "  43:         # padding object\n",
      "  44:         while len(obj_type) != obj_max:\n",
      "  45:             obj_type.append(0)\n",
      "  46:         \n",
      "  47:         label = bag['Y']\n",
      "  48:         \n",
      "  49:         while len(label) > 1:\n",
      "  50:             label.pop()\n",
      "  51:         \n",
      "  52:         dataset.append([pad_pos1, pad_pos2, embeds, pad_alias, sub_type, obj_type, label])\n",
      "  53:         countBags += 1\n",
      "  54:         \n",
      "  55:     return dataset, countBags, aliases_count\n",
      "  56: \n",
      "  57: def getData(data, max_pos):\n",
      "  58:     max_sent_size = max_pos\n",
      "  59:     max_sent_in_bag, max_alias, max_alias_len, subj_max, obj_max, aliases_count = 0, 0, 0, 0, 0, 0 \n",
      "  60:     \n",
      "  61:     for i, bag in enumerate(data):\n",
      "  62:         embeds = bag['embeds']\n",
      "  63:         aliases = bag['ProbY']\n",
      "  64:         sub_type = bag['SubType']\n",
      "  65:         obj_type = bag['ObjType']\n",
      "  66:         pos1 = bag['Pos1']\n",
      "  67:         pos2 = bag['Pos2']\n",
      "  68:         \n",
      "  69:         for p in pos1:\n",
      "  70:             max_sent_size = max(max_sent_size, len(p))\n",
      "  71:             \n",
      "  72:         for p in pos2:\n",
      "  73:             max_sent_size = max(max_sent_size, len(p))\n",
      "  74:         \n",
      "  75:         max_sent_in_bag = max(max_sent_in_bag, len(embeds))\n",
      "  76:         \n",
      "  77:         max_alias = max(max_alias, len(aliases))\n",
      "  78:         \n",
      "  79:         for alias in aliases:\n",
      "  80:             max_alias_len = max(max_alias_len, len(alias))\n",
      "  81:             for a in alias:\n",
      "  82:                 aliases_count = max(aliases_count, a)\n",
      "  83:             \n",
      "  84:         subj_max = max(subj_max, len(sub_type))\n",
      "  85:         obj_max = max(obj_max, len(obj_type))\n",
      "  86:     \n",
      "  87:     #print(\"MAX_SENT_IN_BAG: %d\" % max_sent_in_bag)\n",
      "  88:     #print(\"MAX_ALIAS: %d\" % max_alias)\n",
      "  89:     #print(\"MAX_ALIAS_LEN: %d\" % max_alias_len)\n",
      "  90:     #print(\"SUBJ_MAX: %d\" % subj_max)\n",
      "  91:     #print(\"aliases_count: %d\\n\" % aliases_count)\n",
      "  92:     \n",
      "  93:     return padData(data, max_sent_in_bag, max_alias, max_alias_len, subj_max, obj_max, max_sent_size, aliases_count+1)\n",
      "  94: \n",
      "  95: def get_chunks(data, batch_size):\n",
      "  96:     count_bags = len(data)\n",
      "  97:     for indx in range(0, count_bags, batch_size):\n",
      "  98:         yield data[indx:min(indx + batch_size, count_bags)]\n",
      "  99: \n",
      " 100: def get_batches(batches):\n",
      " 101:     while True:\n",
      " 102:         for batch in batches:\n",
      " 103:             yield ({'subjpos': np.array(batch[0], dtype='int32'), \n",
      " 104:                     'objpos': np.array(batch[1], dtype='int32'), \n",
      " 105:                     'bert_input': np.array(batch[2], dtype='float32'), \n",
      " 106:                     'alias': np.array(batch[3], dtype='int32'), \n",
      " 107:                     'subj': np.array(batch[4], dtype='int32'), \n",
      " 108:                     'obj': np.array(batch[5], dtype='int32')}, \n",
      " 109:                    {'output': np.array(batch[6], dtype='int32')})\n",
      " 110: \n",
      " 111: def create_batches(data, batch_size=64):\n",
      " 112:     batches = []\n",
      " 113:     for chunk in get_chunks(data, batch_size):\n",
      " 114:         batch = ddict(list)\n",
      " 115:         num = 0\n",
      " 116:         \n",
      " 117:         pos1 = []\n",
      " 118:         pos2 = []\n",
      " 119:         bert = []\n",
      " 120:         alias = []\n",
      " 121:         subj = []\n",
      " 122:         obj = []\n",
      " 123:         out = []\n",
      " 124:         for i, bag in enumerate(chunk):\n",
      " 125:             pos1.append(bag[0])\n",
      " 126:             pos2.append(bag[1])\n",
      " 127:             bert.append(bag[2])\n",
      " 128:             alias.append(bag[3])\n",
      " 129:             subj.append(bag[4])\n",
      " 130:             obj.append(bag[5])\n",
      " 131:             out.append(bag[6])\n",
      " 132:         batches.append([pos1, pos2, bert, alias, subj, obj, out])\n",
      " 133:         \n",
      " 134:     return batches, len(batches)\n",
      " 135: \n",
      " 136: \n",
      ">>> Data\n",
      "  1: \n",
      "  2: batch_size = 512\n",
      "  3: type_dim = 150\n",
      "  4: alias_dim = 150\n",
      "  5: embedding_dim = 768\n",
      "  6: L2 = 0.001\n",
      "  7: regularizer = tf.keras.regularizers.l2(l=0.5 * (L2))\n",
      "  8: \n",
      "  9: data = pickle.load(open('data/gids_bert_processed-test.pkl', 'rb'))\n",
      " 10: \n",
      " 11: voc2id      = data['voc2id']\n",
      " 12: id2voc      = data['id2voc']\n",
      " 13: type2id     = data['type2id']\n",
      " 14: type_count  = len(data['type2id'])\n",
      " 15: num_class   = len(data['rel2id'])\n",
      " 16: max_pos = data['max_pos']\n",
      " 17: \n",
      " 18: print(\"getData Train\")\n",
      " 19: train, train_bags, alias_count = getData(data['train'], max_pos)\n",
      " 20: \n",
      " 21: print(\"getData Test\")\n",
      " 22: validation, test_bags, alias_count = getData(data['dev'], max_pos)\n",
      " 23: \n",
      " 24: print(\"getData Validation\")\n",
      " 25: test, dev_bags, alias_count = getData(data['test'], max_pos)\n",
      " 26: \n",
      " 27: \n",
      " 28: \n",
      " 29: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \n",
      "   4:     # embed input\n",
      "   5:     embed_input = tf.keras.Input(shape=(None, embedding_dim,), name=\"bert_input\", dtype='float32')\n",
      "   6:     \n",
      "   7:     # embed alias\n",
      "   8:     alias_input = tf.keras.Input(shape=(None, None,), name=\"alias\", dtype='int32')\n",
      "   9:     embed_alias = tf.keras.layers.Embedding(input_dim=alias_count, \n",
      "  10:                                             output_dim=alias_dim, trainable=True,\n",
      "  11:                                             embeddings_initializer=tf.keras.initializers.VarianceScaling(\n",
      "  12:                                                 scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"), \n",
      "  13:                                             embeddings_regularizer=regularizer, name='alias_embed')(alias_input)\n",
      "  14:     \n",
      "  15:     alias_av = tf.math.reduce_sum(input_tensor=embed_alias, axis=2, keepdims=False, name='alias_mean') / tf.cast(tf.shape(embed_alias)[1], tf.float32)\n",
      "  16:     \n",
      "  17:     '''pos1_input = tf.keras.Input(shape=(max_pos,), name=\"subjpos\", dtype='float32')\n",
      "  18:     \n",
      "  19:     embed_pos1 = tf.keras.layers.Embedding(input_dim=max_pos+1,\n",
      "  20:                                           output_dim=alias_dim, trainable=True,\n",
      "  21:                                           embeddings_initializer=tf.keras.initializers.VarianceScaling(\n",
      "  22:                                               scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"), \n",
      "  23:                                           embeddings_regularizer=regularizer, name='pos_embed1')(pos1_input)\n",
      "  24:     \n",
      "  25:     pos1_embed_reduced = tf.math.reduce_mean(input_tensor=embed_pos1, axis=1, keepdims=True, name='pos')\n",
      "  26:     \n",
      "  27:     \n",
      "  28:     bert_flatten = tf.keras.layers.Flatten()(embed_input)\n",
      "  29:     alias_flatten = tf.keras.layers.Flatten()(alias_av)\n",
      "  30:     pos1_flatten = tf.keras.layers.Flatten()(pos1_embed_reduced)\n",
      "  31:     '''\n",
      "  32:     # sentence representations\n",
      "  33:     sent_reps = tf.keras.layers.concatenate([embed_input, alias_av], axis=2, name='bert_alias_concat')\n",
      "  34:     \n",
      "  35:     \n",
      "  36:     #a = tf.keras.backend.squeeze(embed_pos1, axis=0)\n",
      "  37:     #lstm = tf.keras.layers.LSTM(64)(a)\n",
      "  38:     \n",
      "  39:     ''' / tf.cast(tf.shape(embed_pos1)[2], tf.float32)\n",
      "  40:         \n",
      "  41:     m = tf.keras.Input(shape=(None, None, ), name=\"objpos\", dtype='int32')\n",
      "  42:     \n",
      "  43:     embed_pos2 = tf.keras.layers.Embedding(input_dim=max_pos,\n",
      "  44:                                           output_dim=alias_dim, \n",
      "  45:                                           embeddings_initializer=tf.keras.initializers.VarianceScaling(\n",
      "  46:                                               scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"), \n",
      "  47:                                           embeddings_regularizer=regularizer, name='pos_embed2')(m)\n",
      "  48:     \n",
      "  49:     \n",
      "  50:     pos2_embed = tf.math.reduce_sum(input_tensor=embed_pos2, \n",
      "  51:                                                axis=2, keepdims=False, name='pos2_sum')'''\n",
      "  52:     \n",
      "  53:     \n",
      "  54:     # Input subj-obj types\n",
      "  55:     subj_input = tf.keras.layers.Input(shape=(None,), name=\"subj\", dtype='int32')\n",
      "  56:     obj_input = tf.keras.layers.Input(shape=(None,), name=\"obj\", dtype='int32')\n",
      "  57:     \n",
      "  58:     # embed subj-obj types\n",
      "  59:     embed_type = tf.keras.layers.Embedding(input_dim=type_count, output_dim=type_dim, \n",
      "  60:                                            embeddings_regularizer=regularizer,\n",
      "  61:                                            embeddings_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
      "  62:                                                scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"), name='type_embed')\n",
      "  63:     subj_embed = embed_type(subj_input)\n",
      "  64:     obj_embed = embed_type(obj_input)\n",
      "  65:     \n",
      "  66:     # average of types representations\n",
      "  67:     subj_type_av = tf.math.reduce_mean(input_tensor=subj_embed, axis=1, name='subj_mean')\n",
      "  68:     obj_type_av = tf.math.reduce_mean(input_tensor=obj_embed, axis=1, name='obj_mean')\n",
      "  69:     \n",
      "  70:     # concatenate subject and object to one single representation\n",
      "  71:     concat_type = tf.keras.layers.concatenate([subj_type_av, obj_type_av], axis=1, name='type_concat')\n",
      "  72:     \n",
      "  73:     # weights for querying attention layer \n",
      "  74:     # samples are drawn from a uniform distribution within [-limit, limit], with limit = sqrt(3 * scale / n)\n",
      "  75:     sent_atten_q = tf.keras.initializers.VarianceScaling(scale=1.0, \n",
      "  76:                                                          mode=\"fan_avg\", \n",
      "  77:                                                          distribution=\"uniform\")(shape=(1, 1, alias_dim+embedding_dim))  # alias_dim+\n",
      "  78:     \n",
      "  79:     # Bag Attention sentence level\n",
      "  80:     bag_attention = tf.keras.layers.Attention(name='attention')([sent_atten_q, sent_reps])\n",
      "  81:     \n",
      "  82:     mean_bag_attention = tf.math.reduce_mean(bag_attention, axis=1)\n",
      "  83:     \n",
      "  84:     bag_reps = tf.keras.layers.concatenate([mean_bag_attention, concat_type], axis=1, name='bag_rep_type_concat')\n",
      "  85:     \n",
      "  86:     # fully connected\n",
      "  87:     fc1 = tf.keras.layers.Dense(units=space['units'], \n",
      "  88:                                 activation=\"relu\", \n",
      "  89:                                 kernel_regularizer=regularizer, name='fully1')(bag_reps)\n",
      "  90:     \n",
      "  91:     drop1 = tf.keras.layers.Dropout(rate=space['rate'], name='drop1')(fc1)\n",
      "  92:     \n",
      "  93:     fc2 = tf.keras.layers.Dense(units=space['units_1'], \n",
      "  94:                                 activation=\"relu\", \n",
      "  95:                                 kernel_regularizer=regularizer, name='fully2')(drop1)\n",
      "  96:     \n",
      "  97:     drop = tf.keras.layers.Dropout(rate=space['rate_1'], name='drop2')(fc2)\n",
      "  98:     '''\n",
      "  99:     hidden_layers_number = space['hidden_layers_number']\n",
      " 100:     \n",
      " 101:     if hidden_layers_number == 'two':\n",
      " 102:         fc2 = tf.keras.layers.Dense(units=space['units_2'], \n",
      " 103:                                 activation=space['activation'], \n",
      " 104:                                 kernel_regularizer=regularizer, name='fully2')(drop)\n",
      " 105:         drop = tf.keras.layers.Dropout(rate=space['rate_2'], name='drop2')(fc2)\n",
      " 106:     elif hidden_layers_number == 'three':\n",
      " 107:         fc2 = tf.keras.layers.Dense(units=space['units_3'], \n",
      " 108:                                 activation=space['activation_1'], \n",
      " 109:                                 kernel_regularizer=regularizer, name='fully2')(drop)\n",
      " 110:         drop2 = tf.keras.layers.Dropout(rate=space['rate_3'], name='drop2')(fc2)\n",
      " 111:         \n",
      " 112:         fc3 = tf.keras.layers.Dense(units=space['units_4'], \n",
      " 113:                                 activation=space['activation_2'], \n",
      " 114:                                 kernel_regularizer=regularizer, name='fully3')(drop2)\n",
      " 115:         drop = tf.keras.layers.Dropout(rate=space['rate_4'], name='drop3')(fc3)'''\n",
      " 116:     \n",
      " 117:     # output layer\n",
      " 118:     label = tf.keras.layers.Dense(num_class, activation='softmax', kernel_regularizer=regularizer, name='output')(drop)\n",
      " 119:     \n",
      " 120:     model = tf.keras.Model([embed_input, alias_input, subj_input, obj_input], label)\n",
      " 121:     \n",
      " 122:     print(model.summary())\n",
      " 123:     \n",
      " 124:     #nadam = tf.keras.optimizers.Nadam(lr=space['rate_5'], name='Nadam')\n",
      " 125:     sgd = tf.keras.optimizers.SGD(lr=space['rate_6'], name='SGD')\n",
      " 126: \n",
      " 127:     '''choiceval = space['choiceval']\n",
      " 128:     if choiceval == 'nadam':\n",
      " 129:         optim = nadam\n",
      " 130:     elif choiceval == 'sgd':'''\n",
      " 131:     optim = sgd\n",
      " 132:     \n",
      " 133:     \n",
      " 134:     model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
      " 135:                   optimizer=optim,\n",
      " 136:                   metrics=['accuracy'])\n",
      " 137:     \n",
      " 138:     train_batches, train_batches_len = create_batches(train, batch_size)\n",
      " 139:     val_batches, val_batches_len = create_batches(validation, batch_size)\n",
      " 140:     test_batches, test_batches_len = create_batches(test, batch_size)\n",
      " 141:     \n",
      " 142:     \n",
      " 143:     checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath=\"models/es_best.h5\", verbose=1, \n",
      " 144:                                    save_best_only=True, save_weights_only=True)\n",
      " 145:     earlystopper = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=40, verbose=1)\n",
      " 146:     \n",
      " 147:     history = model.fit_generator(get_batches(train_batches), steps_per_epoch=train_batches_len, \n",
      " 148:                         epochs=1000, verbose=2, validation_steps=val_batches_len, \n",
      " 149:                         validation_data=get_batches(val_batches), callbacks=[checkpointer, earlystopper])\n",
      " 150:     \n",
      " 151:     score, acc = model.evaluate_generator(generator=get_batches(test_batches), \n",
      " 152:                                           steps=test_batches_len, verbose=0)\n",
      " 153:     print('Test accuracy:', acc)\n",
      " 154:     return {'loss': -acc, 'status': STATUS_OK, 'model': model, \n",
      " 155:             'history.val_loss':history.history['val_loss'], \n",
      " 156:             'history.val_acc': history.history['val_accuracy'],\n",
      " 157:             'history.loss': history.history['loss'], \n",
      " 158:             'history.acc': history.history['accuracy']}\n",
      " 159: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getData Train\n",
      "getData Test\n",
      "getData Validation\n",
      "Model: \"model_2\"                                    \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "alias (InputLayer)              [(None, None, None)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "alias_embed (Embedding)         (None, None, None, 1 750         alias[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_12 (TensorFlo [(4,)]               0           alias_embed[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [()]                 0           tf_op_layer_Shape_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_alias_mean_12 (Tens [(None, None, 150)]  0           alias_embed[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Cast_12 (TensorFlow [()]                 0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "bert_input (InputLayer)         [(None, None, 768)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_truediv_12 (TensorF [(None, None, 150)]  0           tf_op_layer_alias_mean_12[0][0]  \n",
      "                                                                 tf_op_layer_Cast_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bert_alias_concat (Concatenate) (None, None, 918)    0           bert_input[0][0]                 \n",
      "                                                                 tf_op_layer_truediv_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_11 (TensorFl [(None, 1, None)]    0           bert_alias_concat[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "subj (InputLayer)               [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "obj (InputLayer)                [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softmax_5 (TensorFl [(None, 1, None)]    0           tf_op_layer_MatMul_11[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "type_embed (Embedding)          (None, None, 150)    3750        subj[0][0]                       \n",
      "                                                                 obj[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_12 (TensorFl [(None, 1, 918)]     0           tf_op_layer_Softmax_5[0][0]      \n",
      "                                                                 bert_alias_concat[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_subj_mean_6 (Tensor [(None, 150)]        0           type_embed[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_obj_mean_6 (TensorF [(None, 150)]        0           type_embed[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean_3 (TensorFlowO [(None, 918)]        0           tf_op_layer_MatMul_12[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "type_concat (Concatenate)       (None, 300)          0           tf_op_layer_subj_mean_6[0][0]    \n",
      "                                                                 tf_op_layer_obj_mean_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "bag_rep_type_concat (Concatenat (None, 1218)         0           tf_op_layer_Mean_3[0][0]         \n",
      "                                                                 type_concat[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "fully1 (Dense)                  (None, 60)           73140       bag_rep_type_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "drop1 (Dropout)                 (None, 60)           0           fully1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fully2 (Dense)                  (None, 40)           2440        drop1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "drop2 (Dropout)                 (None, 40)           0           fully2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 5)            205         drop2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 80,285                                \n",
      "Trainable params: 80,285                            \n",
      "Non-trainable params: 0                             \n",
      "__________________________________________________________________________________________________\n",
      "None                                                \n",
      "Epoch 1/1000                                        \n",
      "                                                    \n",
      "Epoch 00001: val_loss improved from inf to 1.42246, saving model to models/es_best.h5\n",
      "13/13 - 16s - loss: 1.5282 - accuracy: 0.3586 - val_loss: 1.4225 - val_accuracy: 0.3784\n",
      "\n",
      "Epoch 2/1000                                        \n",
      "                                                    \n",
      "Epoch 00002: val_loss improved from 1.42246 to 1.35184, saving model to models/es_best.h5\n",
      "13/13 - 11s - loss: 1.4485 - accuracy: 0.3680 - val_loss: 1.3518 - val_accuracy: 0.3784\n",
      "\n",
      "Epoch 3/1000                                        \n",
      "                                                    \n",
      "Epoch 00003: val_loss did not improve from 1.35184\n",
      "13/13 - 12s - loss: 1.3864 - accuracy: 0.3837 - val_loss: 1.5771 - val_accuracy: 0.3784\n",
      "\n",
      "Epoch 4/1000                                        \n",
      "                                                    \n",
      "Epoch 00004: val_loss improved from 1.35184 to 1.26275, saving model to models/es_best.h5\n",
      "13/13 - 10s - loss: 1.4025 - accuracy: 0.3934 - val_loss: 1.2627 - val_accuracy: 0.4283\n",
      "\n",
      "Epoch 5/1000                                        \n",
      "                                                    \n",
      "Epoch 00005: val_loss improved from 1.26275 to 1.11264, saving model to models/es_best.h5\n",
      "13/13 - 11s - loss: 1.2829 - accuracy: 0.4331 - val_loss: 1.1126 - val_accuracy: 0.6105\n",
      "\n",
      "Epoch 6/1000                                        \n",
      "                                                    \n",
      "Epoch 00006: val_loss improved from 1.11264 to 1.03096, saving model to models/es_best.h5\n",
      "13/13 - 11s - loss: 1.1427 - accuracy: 0.5272 - val_loss: 1.0310 - val_accuracy: 0.6152\n",
      "\n",
      "Epoch 7/1000                                        \n",
      "                                                    \n",
      "Epoch 00007: val_loss improved from 1.03096 to 0.86802, saving model to models/es_best.h5\n",
      "13/13 - 11s - loss: 1.0560 - accuracy: 0.5823 - val_loss: 0.8680 - val_accuracy: 0.6910\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000                                        \n",
      "                                                    \n",
      "Epoch 00008: val_loss improved from 0.86802 to 0.81369, saving model to models/es_best.h5\n",
      "13/13 - 11s - loss: 0.9479 - accuracy: 0.6269 - val_loss: 0.8137 - val_accuracy: 0.7049\n",
      "\n",
      "Epoch 9/1000                                        \n",
      "                                                    \n",
      "Epoch 00009: val_loss did not improve from 0.81369\n",
      "13/13 - 10s - loss: 0.9355 - accuracy: 0.6254 - val_loss: 0.8991 - val_accuracy: 0.6031\n",
      "\n",
      "Epoch 10/1000                                       \n",
      "                                                    \n",
      "Epoch 00010: val_loss improved from 0.81369 to 0.78667, saving model to models/es_best.h5\n",
      "13/13 - 11s - loss: 0.9374 - accuracy: 0.6331 - val_loss: 0.7867 - val_accuracy: 0.7095\n",
      "\n",
      "Epoch 11/1000                                       \n",
      "                                                    \n",
      "Epoch 00011: val_loss did not improve from 0.78667\n",
      "13/13 - 11s - loss: 0.8901 - accuracy: 0.6531 - val_loss: 0.8306 - val_accuracy: 0.6735\n",
      "\n",
      "Epoch 12/1000                                       \n",
      "                                                    \n",
      "Epoch 00012: val_loss improved from 0.78667 to 0.74975, saving model to models/es_best.h5\n",
      "13/13 - 11s - loss: 0.9403 - accuracy: 0.6382 - val_loss: 0.7498 - val_accuracy: 0.7160\n",
      "\n",
      "Epoch 13/1000                                       \n",
      "                                                    \n",
      "Epoch 00013: val_loss did not improve from 0.74975\n",
      "13/13 - 10s - loss: 0.8789 - accuracy: 0.6627 - val_loss: 0.7900 - val_accuracy: 0.7068\n",
      "\n",
      "Epoch 14/1000                                       \n",
      "                                                    \n",
      "Epoch 00014: val_loss did not improve from 0.74975\n",
      "13/13 - 10s - loss: 0.8297 - accuracy: 0.6901 - val_loss: 0.7609 - val_accuracy: 0.7086\n",
      "\n",
      "Epoch 15/1000                                       \n",
      "                                                    \n",
      "Epoch 00015: val_loss improved from 0.74975 to 0.69598, saving model to models/es_best.h5\n",
      "13/13 - 11s - loss: 0.8093 - accuracy: 0.6984 - val_loss: 0.6960 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 16/1000                                       \n",
      "                                                    \n",
      "Epoch 00016: val_loss did not improve from 0.69598\n",
      "13/13 - 11s - loss: 0.8308 - accuracy: 0.6831 - val_loss: 0.7204 - val_accuracy: 0.7345\n",
      "\n",
      "Epoch 17/1000                                       \n",
      "                                                    \n",
      "Epoch 00017: val_loss did not improve from 0.69598\n",
      "13/13 - 10s - loss: 0.7889 - accuracy: 0.7019 - val_loss: 0.6972 - val_accuracy: 0.7290\n",
      "\n",
      "Epoch 18/1000                                       \n",
      "                                                    \n",
      "Epoch 00018: val_loss did not improve from 0.69598\n",
      "13/13 - 10s - loss: 0.8358 - accuracy: 0.6761 - val_loss: 0.7013 - val_accuracy: 0.7410\n",
      "\n",
      "Epoch 19/1000                                       \n",
      "                                                    \n",
      "Epoch 00019: val_loss did not improve from 0.69598\n",
      "13/13 - 10s - loss: 0.7657 - accuracy: 0.7142 - val_loss: 0.7064 - val_accuracy: 0.7317\n",
      "\n",
      "Epoch 20/1000                                       \n",
      "                                                    \n",
      "Epoch 00020: val_loss did not improve from 0.69598\n",
      "13/13 - 10s - loss: 0.7890 - accuracy: 0.7015 - val_loss: 0.7456 - val_accuracy: 0.7188\n",
      "\n",
      "Epoch 21/1000                                       \n",
      "                                                    \n",
      "Epoch 00021: val_loss did not improve from 0.69598\n",
      "13/13 - 11s - loss: 0.7713 - accuracy: 0.7084 - val_loss: 0.7870 - val_accuracy: 0.6864\n",
      "\n",
      "Epoch 22/1000                                       \n",
      "                                                    \n",
      "Epoch 00022: val_loss did not improve from 0.69598\n",
      "13/13 - 11s - loss: 0.7844 - accuracy: 0.7028 - val_loss: 0.7672 - val_accuracy: 0.7012\n",
      "\n",
      "Epoch 23/1000                                       \n",
      "  0%|          | 0/10 [04:03<?, ?it/s, best loss: ?]"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=10,\n",
    "                                      trials=trials,\n",
    "                                      functions=[padData, getData, get_chunks, get_batches, create_batches],\n",
    "                                      notebook_name='tf-test-new')\n",
    "\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)\n",
    "\n",
    "tf.keras.utils.plot_model(best_model, to_file='models/best_model-03-12.png', show_shapes=True, show_layer_names=True)\n",
    "best_model.save('models/best_model-03-12.h5')\n",
    "\n",
    "#print(trials.trials)\n",
    "#print(\"\\n\")\n",
    "best_trial_ = trials.best_trial\n",
    "\n",
    "#print(best_trial_)\n",
    "\n",
    "acc = best_trial_.get('result').get('history.acc')\n",
    "loss = best_trial_.get('result').get('history.loss')\n",
    "val_acc = best_trial_.get('result').get('history.val_acc')\n",
    "val_loss = best_trial_.get('result').get('history.val_loss')\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(acc)\n",
    "plt.plot(val_acc)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "df = pd.DataFrame.from_dict({'acc':acc, 'loss': loss, 'acc_val': val_acc, 'acc_loss': val_loss})\n",
    "\n",
    "df.to_pickle('trainHistoryOld.pkl')\n",
    "\n",
    "# \n",
    "plt.plot(acc)\n",
    "plt.plot(val_acc)\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.title('model loss/accuracy across epochs')\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['acc', 'val_acc', 'loss', 'val_loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_label(dataset):\n",
    "    batches = []\n",
    "    labels = []\n",
    "    for d in dataset:\n",
    "        batches.append([np.array(d[0], dtype='float32'), np.array(d[1], dtype='int32'), \n",
    "                        np.array(d[2], dtype='int32'), np.array(d[3], dtype='int32')])\n",
    "        labels.append(d[4])\n",
    "        \n",
    "    return np.array(batches), np.array(labels)\n",
    "\n",
    "def getPdata(data):\n",
    "    \n",
    "    p_one = []\n",
    "    p_two = []\n",
    "    \n",
    "    for bag in data['test']:\n",
    "        if len(bag['X']) < 2: continue\n",
    "        \n",
    "        indx = list(range(len(bag['X'])))\n",
    "        random.shuffle(indx)\n",
    "        \n",
    "        p_one.append({\n",
    "            'embeds':   [bag['embeds'][indx[0]]],\n",
    "            'ProbY': \t[bag['ProbY'][indx[0]]],\n",
    "            'Y':    \tbag['Y'],\n",
    "            'SubType':\tbag['SubType'],\n",
    "            'ObjType':\tbag['ObjType']\n",
    "        })\n",
    "\n",
    "        p_two.append({\n",
    "            'embeds':   [bag['embeds'][indx[0]], bag['embeds'][indx[1]]],\n",
    "            'ProbY': \t[bag['ProbY'][indx[0]], bag['ProbY'][indx[1]]],\n",
    "            'Y':   \t \tbag['Y'],\n",
    "            'SubType':\tbag['SubType'],\n",
    "            'ObjType':\tbag['ObjType']\n",
    "        })\n",
    "    \n",
    "    \n",
    "    return getData(p_one), getData(p_two)\n",
    "\n",
    "\n",
    "def getPscore(p_n_data):\n",
    "    \n",
    "    data_one, y_true = split_label(p_n_data)\n",
    "\n",
    "    y_pred_indx = []\n",
    "    y_pred_prob = []\n",
    "    for bert, alias, subj, obj in data_one:\n",
    "        prediction = model.predict([[bert], [alias], [subj], [obj]])\n",
    "        y_pred_indx.append(np.argmax(prediction))\n",
    "        y_pred_prob.append(np.amax(prediction))\n",
    "    \n",
    "    y_pred_indx = np.array(y_pred_indx)\n",
    "    y_prob = np.reshape(np.array(y_pred_prob), (-1))\n",
    "    y_true = np.reshape(np.array(y_true), (-1))\n",
    "    order = np.argsort(-y_prob)\n",
    "    \n",
    "    def p_score(n):\n",
    "        sum_correct_pred = 0.0\n",
    "        for i in order[:n]:\n",
    "            sum_correct_pred += 1.0 if (y_true[i] == y_pred_indx[i]) else 0\n",
    "        return sum_correct_pred / n\n",
    "        \n",
    "    return p_score(100), p_score(200), p_score(300)\n",
    "\n",
    "def savePredictions2File(dataset_name, algorithm_name, data):\n",
    "    validation, count_bags, alias_count = getData(data['test'])\n",
    "    classes_ = len(data['rel2id'])\n",
    "    \n",
    "    data, y_true = split_label(validation)\n",
    "\n",
    "    logit_list = []\n",
    "    for bert, alias, subj, obj in data:\n",
    "        logit_list.append((model.predict([[bert], [alias], [subj], [obj]])[0]).tolist())\n",
    "\n",
    "    y_flatten = y_true.flatten().tolist()\n",
    "    y_actual_hot = (tf.keras.utils.to_categorical(y_flatten, num_classes=classes_)).tolist()\n",
    "    \n",
    "    pickle.dump({'logit_list': logit_list, 'y_hot': y_actual_hot}, \n",
    "                open(\"results/{}/{}/precision_recall.pkl\".format(dataset_name, algorithm_name), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jms5/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save data to plot precision-recall curve\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('models/best_model-03-12.h5')\n",
    "\n",
    "data = pickle.load(open('data/gids_bert_processed-test.pkl', 'rb'))\n",
    "\n",
    "print(\"Save data to plot precision-recall curve\")\n",
    "\n",
    "savePredictions2File(\"gids\", \"BERT-SIDE\", data)\n",
    "\n",
    "print(\"precision_recall.pkl created\")\n",
    "\n",
    "print(\"\\nP@N results\")\n",
    "\n",
    "one_, two_ = getPdata(data)\n",
    "\n",
    "one, one_bags_count = one_\n",
    "two, two_bags_count = two_\n",
    "\n",
    "print('=============== test one =============================')\n",
    "one_100, one_200, one_300 = getPscore(one)\n",
    "print(\"P@100: {}, P@200: {}, P@300: {}\".format(one_100, one_200, one_300))\n",
    "\n",
    "print('=============== test two =============================')\n",
    "two_100, two_200, two_300 = getPscore(two)\n",
    "print(\"P@100: {}, P@200: {}, P@300: {}\".format(two_100, two_200, two_300))\n",
    "\n",
    "print('=============== test all =============================')\n",
    "all_, count_bags = getData(data['test'])\n",
    "all_100, all_200, all_300 = getPscore(all_)\n",
    "print(\"P@100: {}, P@200: {}, P@300: {}\".format(all_100, all_200, all_300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pickle.load(open('data/gids_bert_processed-test.pkl', 'rb'))\n",
    "\n",
    "bagTest = list()\n",
    "\n",
    "for d in data['test']:\n",
    "    bagTest.append(len(d['X']))\n",
    "\n",
    "\n",
    "counter=collections.Counter(bagTest)\n",
    "\n",
    "print(counter)\n",
    "    \n",
    "plt.bar(counter.keys(), counter.values())\n",
    "plt.ylabel('Bags Count')\n",
    "plt.xlabel('Bags Size')\n",
    "plt.title('Bags Size on Test Set')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n= P @ ONE ==============================================================\\n\")\n",
    "\n",
    "# P@ONE\n",
    "data_one, y_true = split_label(one)\n",
    "\n",
    "y_pred = []\n",
    "for bert, alias, subj, obj in data_one:\n",
    "    y_pred.append(np.argmax(model.predict([[bert], [alias], [subj], [obj]])))\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "print(y_pred.shape)\n",
    "\n",
    "print(metrics.confusion_matrix(y_true, y_pred))\n",
    "\n",
    "print(metrics.classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "print(\"Precision: %.4f\" % metrics.precision_score(y_true, y_pred, average='macro'))\n",
    "print(\"Recall: %.4f\" % metrics.recall_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "print(\"\\n= P @ TWO ==============================================================\\n\")\n",
    "\n",
    "# P@TWO\n",
    "data_two, y_true = split_label(two)\n",
    "\n",
    "y_pred = []\n",
    "for bert, alias, subj, obj in data_two:\n",
    "    y_pred.append(np.argmax(model.predict([[bert], [alias], [subj], [obj]])))\n",
    "    \n",
    "y_pred = np.array(y_pred)\n",
    "print(y_pred.shape)\n",
    "\n",
    "print(metrics.confusion_matrix(y_true, y_pred))\n",
    "\n",
    "print(metrics.classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "print(\"Precision: %.4f\" % metrics.precision_score(y_true, y_pred, average='macro'))\n",
    "print(\"Recall: %.4f\" % metrics.recall_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "print(\"\\n= P @ ALL ==============================================================\\n\")\n",
    "\n",
    "# P@ALL\n",
    "\n",
    "validation, count_bags = getData(data['test'])\n",
    "\n",
    "data, y_true = split_label(validation)\n",
    "\n",
    "y_pred = []\n",
    "for bert, alias, subj, obj in data:\n",
    "    y_pred.append(np.argmax(model.predict([[bert], [alias], [subj], [obj]])))\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "print(y_pred.shape)\n",
    "\n",
    "print(metrics.confusion_matrix(y_true, y_pred))\n",
    "\n",
    "print(metrics.classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "print(\"Precision: %.4f\" % metrics.precision_score(y_true, y_pred, average='macro'))\n",
    "print(\"Recall: %.4f\" % metrics.recall_score(y_true, y_pred, average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
