{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, random, argparse\n",
    "import pickle\n",
    "from collections import defaultdict as ddict\n",
    "import tensorflow as tf\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, randint\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set precision for numpy\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"riedel\"\n",
    "path2datafile = \"data/{}_bert_processed.pkl\".format(dataset_name)\n",
    "models_dir = \"models/\"\n",
    "results_dir = \"results/{}/\".format(dataset_name)\n",
    "\n",
    "model_name = '{}_best_model'.format(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padData(data, max_sent_in_bag, max_alias, max_alias_len, subj_max, obj_max, aliases_count):\n",
    "    \n",
    "    print(\"padding data...\")\n",
    "\n",
    "    for i, bag in enumerate(data):\n",
    "        # padding embeds\n",
    "        while len(bag['embeds']) <= max_sent_in_bag:\n",
    "            bag['embeds'].append(np.zeros(768))\n",
    "\n",
    "        if len(bag['embeds']) > max_sent_in_bag:\n",
    "            bag['embeds'] = bag['embeds'][:max_sent_in_bag]\n",
    "\n",
    "        # padding alias\n",
    "        while len(bag['ProbY']) <= max_sent_in_bag:\n",
    "            bag['ProbY'].append([0])\n",
    "\n",
    "        if len(bag['ProbY']) > max_sent_in_bag:\n",
    "            bag['ProbY'] = bag['ProbY'][:max_sent_in_bag]\n",
    "\n",
    "        pad_alias = np.zeros((len(bag['ProbY']), max_alias_len), np.int32)\n",
    "        for j, alias in enumerate(bag['ProbY']):\n",
    "            pad_alias[j, :len(alias)] = alias[:max_alias_len]\n",
    "        \n",
    "        # padding subject\n",
    "        while len(bag['SubType']) < subj_max:\n",
    "            bag['SubType'].append(0)\n",
    "            \n",
    "        # padding object\n",
    "        while len(bag['ObjType']) < obj_max:\n",
    "            bag['ObjType'].append(0)\n",
    "        \n",
    "        while len(bag['Y']) > 1:\n",
    "            bag['Y'].pop()\n",
    "        \n",
    "        bag['ProbY'] = pad_alias\n",
    "    \n",
    "    return data, len(data), aliases_count\n",
    "\n",
    "def getData(data):\n",
    "    max_sent_in_bag, max_alias, max_alias_len, subj_max, obj_max, aliases_count = 0, 0, 0, 0, 0, 0 \n",
    "    \n",
    "    for i, bag in enumerate(data):\n",
    "        del bag['X']\n",
    "        del bag['DepEdges']\n",
    "        del bag['Pos1']\n",
    "        del bag['Pos2']\n",
    "        del bag['SubPos']\n",
    "        del bag['ObjPos']\n",
    "        bag['embeds'] = bag['embeds'].tolist()\n",
    "\n",
    "        if i==0: print(\"Keys: \", bag.keys())\n",
    "\n",
    "        embeds = bag['embeds']\n",
    "        aliases = bag['ProbY']\n",
    "        \n",
    "        max_sent_in_bag = 5\n",
    "        \n",
    "        max_alias = 5\n",
    "        max_alias_len = 20\n",
    "        for alias in aliases:  # number of relation aliases for each sentence in bag\n",
    "            for a in alias:\n",
    "                aliases_count = max(aliases_count, a)\n",
    "        \n",
    "        subj_max = max(subj_max, len(bag['SubType']))\n",
    "        obj_max = max(obj_max, len(bag['ObjType']))\n",
    "    \n",
    "    print(\"bag:\", max_sent_in_bag)\n",
    "    print(\"alias:\", max_alias)\n",
    "    print(\"max_alias_len:\", max_alias_len)\n",
    "    print(\"alias count\", aliases_count)\n",
    "    print(\"SubType:\", subj_max)\n",
    "    print(\"ObjType:\", obj_max)\n",
    "    \n",
    "    return padData(data, max_sent_in_bag, max_alias, max_alias_len, subj_max, obj_max, aliases_count+1)\n",
    "\n",
    "def get_chunks(data, batch_size):\n",
    "    count_bags = len(data)\n",
    "    for indx in range(0, count_bags, batch_size):\n",
    "        yield data[indx:min(indx + batch_size, count_bags)]\n",
    "        \n",
    "def create_batches(data, batch_size):\n",
    "\n",
    "    batches = []\n",
    "    for chunk in get_chunks(data, batch_size):\n",
    "        batch = ddict(list)\n",
    "        num = 0\n",
    "        \n",
    "        bert = []\n",
    "        alias = []\n",
    "        subj = []\n",
    "        obj = []\n",
    "        out = []\n",
    "        \n",
    "        for i, bag in enumerate(chunk):\n",
    "            bert.append(bag['embeds'])\n",
    "            alias.append(bag['ProbY'])\n",
    "            subj.append(bag['SubType'])\n",
    "            obj.append(bag['ObjType'])\n",
    "            out.append(bag['Y'])\n",
    "        batches.append([bert, alias, subj, obj, out])\n",
    "        \n",
    "    return batches, len(batches)\n",
    "    \n",
    "def get_batches(batches):\n",
    "    while True:\n",
    "        for batch in batches:\n",
    "            yield ({'bert_input': np.array(batch[0], dtype='float32'), \n",
    "                    'alias': np.array(batch[1], dtype='int32'), \n",
    "                    'subj': np.array(batch[2], dtype='int32'), \n",
    "                    'obj': np.array(batch[3], dtype='int32')}, \n",
    "                   {'output': np.array(batch[4], dtype='int32')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(path2datafile, models_dir, model_name):\n",
    "    batch_size = 128\n",
    "    type_dim = 150\n",
    "    alias_dim = 150\n",
    "    embedding_dim = 768\n",
    "    L2 = 0.001\n",
    "    regularizer = tf.keras.regularizers.l2(l=0.5 * (L2))\n",
    "    \n",
    "    print(\"loading dataset...\")\n",
    "    \n",
    "    data = pickle.load(open(path2datafile, 'rb'))\n",
    "    \n",
    "    type_count  = len(data['type2id'])\n",
    "    num_class   = len(data['rel2id'])\n",
    "    \n",
    "    print(\"getData Train\")\n",
    "    train, train_bags, alias_count = getData(data['train'])\n",
    "    \n",
    "    print(\"getData Validation\")\n",
    "    test, test_bags, alias_count = getData(data['test'])\n",
    "    \n",
    "    print(\"getData Test\")\n",
    "    dev, dev_bags, alias_count = getData(data['dev'])\n",
    "    \n",
    "    data['train'] = train\n",
    "    data['test'] = test\n",
    "    data['dev'] = dev\n",
    "\n",
    "    return data, embedding_dim, num_class, alias_dim, regularizer, type_count, type_dim, batch_size, alias_count, models_dir, model_name\n",
    "\n",
    "\n",
    "def model(data, embedding_dim, num_class, alias_dim, regularizer, \n",
    "          type_count, type_dim, batch_size, max_pos, alias_count, models_dir, model_name):\n",
    "    \n",
    "    # embed input\n",
    "    embed_input = tf.keras.Input(shape=(None, embedding_dim,), name=\"bert_input\", dtype='float32')\n",
    "    \n",
    "    # embed alias\n",
    "    alias_input = tf.keras.Input(shape=(None, None,), name=\"alias\", dtype='int32')\n",
    "    embed_alias = tf.keras.layers.Embedding(input_dim=alias_count, \n",
    "                                            output_dim=alias_dim, trainable=True,\n",
    "                                            embeddings_initializer=tf.keras.initializers.VarianceScaling(\n",
    "                                                scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"), \n",
    "                                            embeddings_regularizer=regularizer, name='alias_embed')(alias_input)\n",
    "    \n",
    "    alias_av = tf.math.reduce_sum(input_tensor=embed_alias, axis=2, keepdims=False, name='alias_mean') / tf.cast(tf.shape(embed_alias)[1], tf.float32)\n",
    "    \n",
    "    # sentence representations\n",
    "    sent_reps = tf.keras.layers.concatenate([embed_input, alias_av], axis=2, name='bert_alias_concat')\n",
    "    \n",
    "    \n",
    "    # Input subj-obj types\n",
    "    subj_input = tf.keras.layers.Input(shape=(None,), name=\"subj\", dtype='int32')\n",
    "    obj_input = tf.keras.layers.Input(shape=(None,), name=\"obj\", dtype='int32')\n",
    "    \n",
    "    # embed subj-obj types\n",
    "    embed_type = tf.keras.layers.Embedding(input_dim=type_count, output_dim=type_dim, \n",
    "                                           embeddings_regularizer=regularizer,\n",
    "                                           embeddings_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
    "                                               scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"), name='type_embed')\n",
    "    subj_embed = embed_type(subj_input)\n",
    "    obj_embed = embed_type(obj_input)\n",
    "    \n",
    "    # average of types representations\n",
    "    subj_type_av = tf.math.reduce_mean(input_tensor=subj_embed, axis=1, name='subj_mean')\n",
    "    obj_type_av = tf.math.reduce_mean(input_tensor=obj_embed, axis=1, name='obj_mean')\n",
    "    \n",
    "    # concatenate subject and object to one single representation\n",
    "    concat_type = tf.keras.layers.concatenate([subj_type_av, obj_type_av], axis=1, name='type_concat')\n",
    "    \n",
    "    # weights for querying attention layer \n",
    "    # samples are drawn from a uniform distribution within [-limit, limit], with limit = sqrt(3 * scale / n)\n",
    "    sent_atten_q = tf.keras.initializers.VarianceScaling(scale=1.0, \n",
    "                                                         mode=\"fan_avg\", \n",
    "                                                         distribution=\"uniform\")(shape=(1, 1, alias_dim+embedding_dim))  # alias_dim+\n",
    "    \n",
    "    # Bag Attention sentence level\n",
    "    bag_attention = tf.keras.layers.Attention(name='attention')([sent_atten_q, sent_reps])\n",
    "    \n",
    "    mean_bag_attention = tf.math.reduce_mean(bag_attention, axis=1)\n",
    "    \n",
    "    bag_reps = tf.keras.layers.concatenate([mean_bag_attention, concat_type], axis=1, name='bag_rep_type_concat')\n",
    "    \n",
    "    # fully connected\n",
    "    fc1 = tf.keras.layers.Dense(units={{choice([48, 96, 192, 384, 768])}}, \n",
    "                                activation=\"relu\", \n",
    "                                kernel_regularizer=regularizer, name='fully1')(bag_reps)\n",
    "    \n",
    "    drop1 = tf.keras.layers.Dropout(rate={{uniform(0,1)}}, name='drop1')(fc1)\n",
    "    \n",
    "    fc2 = tf.keras.layers.Dense(units={{choice([6, 12, 24, 48])}}, \n",
    "                                activation=\"relu\", \n",
    "                                kernel_regularizer=regularizer, name='fully2')(drop1)\n",
    "    \n",
    "    drop = tf.keras.layers.Dropout(rate={{uniform(0,1)}}, name='drop2')(fc2)\n",
    "    \n",
    "    # output layer\n",
    "    label = tf.keras.layers.Dense(num_class, activation='softmax', kernel_regularizer=regularizer, name='output')(drop)\n",
    "    \n",
    "    model = tf.keras.Model([embed_input, alias_input, subj_input, obj_input], label)\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    optim = tf.keras.optimizers.SGD(lr={{uniform(0,1)}}, name='SGD')\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                  optimizer=optim,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    train_batches, train_batches_len = create_batches(data['train'], batch_size)\n",
    "    val_batches, val_batches_len = create_batches(data['dev'], batch_size)\n",
    "    test_batches, test_batches_len = create_batches(data['test'], batch_size)\n",
    "    \n",
    "    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath=models_dir+ model_name + \"-es_best.h5\", verbose=1, \n",
    "                                   save_best_only=True, save_weights_only=True)\n",
    "    earlystopper = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, verbose=1)\n",
    "    \n",
    "    history = model.fit_generator(get_batches(train_batches), steps_per_epoch=train_batches_len, \n",
    "                        epochs=100, verbose=2, validation_steps=val_batches_len, \n",
    "                        validation_data=get_batches(val_batches), callbacks=[checkpointer, earlystopper])\n",
    "    \n",
    "    score, acc = model.evaluate_generator(generator=get_batches(test_batches), \n",
    "                                          steps=test_batches_len, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    \n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose better parameters to model\n",
    "trials = Trials()\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=3,\n",
    "                                      trials=trials,\n",
    "                                      functions=[padData, getData, get_chunks, get_batches, create_batches],\n",
    "                                      notebook_name='RIEDEL_bert_side',\n",
    "                                      data_args=(path2datafile,models_dir, model_name)\n",
    "                                      )\n",
    "\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)\n",
    "\n",
    "tf.keras.utils.plot_model(best_model, to_file=models_dir + model_name + \".png\", show_shapes=True, show_layer_names=True)\n",
    "best_model.save(models_dir + model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData2(data):\n",
    "    max_sent_in_bag, max_alias, max_alias_len, subj_max, obj_max, aliases_count = 0, 0, 0, 0, 0, 0 \n",
    "    \n",
    "    for i, bag in enumerate(data):\n",
    "        aliases = bag['ProbY']\n",
    "        \n",
    "        max_sent_in_bag = 5\n",
    "        \n",
    "        max_alias = 5 \n",
    "        max_alias_len = 20\n",
    "        for alias in aliases:  # number of relation aliases for each sentence in bag\n",
    "            for a in alias:\n",
    "                aliases_count = max(aliases_count, a)\n",
    "        \n",
    "        subj_max = max(subj_max, len(bag['SubType']))\n",
    "        obj_max = max(obj_max, len(bag['ObjType']))\n",
    "    \n",
    "    print(\"bag:\", max_sent_in_bag)\n",
    "    print(\"alias:\", max_alias)\n",
    "    print(\"max_alias_len:\", max_alias_len)\n",
    "    print(\"alias count\", aliases_count)\n",
    "    print(\"SubType:\", subj_max)\n",
    "    print(\"ObjType:\", obj_max)\n",
    "    \n",
    "    return padData(data, max_sent_in_bag, max_alias, max_alias_len, subj_max, obj_max, aliases_count+1)\n",
    "\n",
    "def split_label(dataset):\n",
    "    batches = []\n",
    "    labels = []\n",
    "    for d in dataset:\n",
    "        batches.append([np.array(d[\"embeds\"], dtype='float32'), np.array(d[\"ProbY\"], dtype='int32'), \n",
    "                        np.array(d[\"SubType\"], dtype='int32'), np.array(d[\"ObjType\"], dtype='int32')])\n",
    "        labels.append(d[\"Y\"])\n",
    "        \n",
    "    return np.array(batches), np.array(labels)\n",
    "\n",
    "def getPdataOne(data):\n",
    "    \n",
    "    p_one = []\n",
    "    \n",
    "    for bag in data:\n",
    "        if len(bag['embeds']) < 2: continue\n",
    "        \n",
    "        indx = list(range(len(bag['embeds'])))\n",
    "        random.shuffle(indx)\n",
    "        \n",
    "        p_one.append({\n",
    "            'embeds':   [bag['embeds'][indx[0]]],\n",
    "            'ProbY': \t[bag['ProbY'][indx[0]]],\n",
    "            'Y':    \tbag['Y'],\n",
    "            'SubType':\tbag['SubType'],\n",
    "            'ObjType':\tbag['ObjType']\n",
    "        })\n",
    "    \n",
    "    return getData2(p_one)\n",
    "\n",
    "def getPdataTwo(data):\n",
    "    \n",
    "    p_two = []\n",
    "    \n",
    "    for bag in data:\n",
    "        if len(bag['embeds']) < 2: continue\n",
    "        \n",
    "        indx = list(range(len(bag['embeds'])))\n",
    "        random.shuffle(indx)\n",
    "\n",
    "        p_two.append({\n",
    "            'embeds':   [bag['embeds'][indx[0]], bag['embeds'][indx[1]]],\n",
    "            'ProbY': \t[bag['ProbY'][indx[0]], bag['ProbY'][indx[1]]],\n",
    "            'Y':   \t \tbag['Y'],\n",
    "            'SubType':\tbag['SubType'],\n",
    "            'ObjType':\tbag['ObjType']\n",
    "        })\n",
    "    \n",
    "    \n",
    "    return getData2(p_two)\n",
    "\n",
    "def getPscore(p_n_data):\n",
    "    \n",
    "    data_one, y_true = split_label(p_n_data)\n",
    "\n",
    "    y_pred_indx = []\n",
    "    y_pred_prob = []\n",
    "    for bert, alias, subj, obj in data_one:\n",
    "        prediction = model.predict([[bert], [alias], [subj], [obj]])\n",
    "        y_pred_indx.append(np.argmax(prediction))\n",
    "        y_pred_prob.append(np.amax(prediction))\n",
    "    \n",
    "    y_pred_indx = np.array(y_pred_indx)\n",
    "    y_prob = np.reshape(np.array(y_pred_prob), (-1))\n",
    "    y_true = np.reshape(np.array(y_true), (-1))\n",
    "    order = np.argsort(-y_prob)\n",
    "    \n",
    "    def p_score(n):\n",
    "        sum_correct_pred = 0.0\n",
    "        for i in order[:n]:\n",
    "            sum_correct_pred += 1.0 if (y_true[i] == y_pred_indx[i]) else 0\n",
    "        return sum_correct_pred / n\n",
    "        \n",
    "    return p_score(100), p_score(200), p_score(300)\n",
    "\n",
    "def savePredictions2File(dataset_name, algorithm_name, data):\n",
    "    validation, count_bags, alias_count = getData(data['test'])\n",
    "    classes_ = len(data['rel2id'])\n",
    "    #print(validation.keys())\n",
    "    data, y_true = split_label(validation)\n",
    "    \n",
    "    logit_list = []\n",
    "    for bert, alias, subj, obj in data:\n",
    "        logit_list.append((model.predict([[bert], [alias], [subj], [obj]])[0]).tolist())\n",
    "\n",
    "    y_flatten = y_true.flatten().tolist()\n",
    "    y_actual_hot = (tf.keras.utils.to_categorical(y_flatten, num_classes=classes_)).tolist()\n",
    "    \n",
    "    pickle.dump({'logit_list': logit_list, 'y_hot': y_actual_hot}, \n",
    "                open(results_dir + \"{}/{}/precision_recall.pkl\".format(dataset_name, algorithm_name), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(models_dir + model_name + '.h5')\n",
    "\n",
    "data = pickle.load(open(path2datafile, 'rb'))\n",
    "\n",
    "data = data['test']\n",
    "\n",
    "print(\"Save data to plot precision-recall curve\")\n",
    "\n",
    "savePredictions2File(\"riedel\", \"BERT-SIDE\", data)\n",
    "\n",
    "print(\"precision_recall.pkl created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nP@N results\")\n",
    "\n",
    "model = tf.keras.models.load_model(models_dir + model_name + '.h5')\n",
    "\n",
    "data = pickle.load(open(path2datafile, 'rb'))\n",
    "\n",
    "data = data['test']\n",
    "\n",
    "one_ = getPdataOne(data)\n",
    "\n",
    "one, one_bags_count, _ = one_\n",
    "\n",
    "print('=============== test one =============================')\n",
    "one_100, one_200, one_300 = getPscore(one)\n",
    "print(\"P@100: {}, P@200: {}, P@300: {}\".format(one_100, one_200, one_300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(path2datafile, 'rb'))\n",
    "\n",
    "data = data['test']\n",
    "\n",
    "two_ = getPdataTwo(data)\n",
    "two, two_bags_count, _ = two_\n",
    "\n",
    "print('=============== test two =============================')\n",
    "two_100, two_200, two_300 = getPscore(two)\n",
    "print(\"P@100: {}, P@200: {}, P@300: {}\".format(two_100, two_200, two_300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(path2datafile, 'rb'))\n",
    "\n",
    "data = data['test']\n",
    "\n",
    "print('=============== test all =============================')\n",
    "all_, count_bags, _ = getData(data)\n",
    "all_100, all_200, all_300 = getPscore(all_)\n",
    "print(\"P@100: {}, P@200: {}, P@300: {}\".format(all_100, all_200, all_300))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
